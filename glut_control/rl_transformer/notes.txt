BENCHMARK
Rather than trying to create an embedded scenario, let's come up with
a sequence of tasks of increasing complexity
  1)  Original left/right scenario with rewards = num(rights)
  2)  Left/right scenario with more complex (and sparse) reward
  3)  Repeat 1 and 2 with 4 dimension
  4)  Navigate a maze?   This was originally suggested, a priori there
  aren't obvious easy places to control glut.  Should I, in general,
  prefer a left turn or right turn?
  5)  Scenario with lots of incoming observations.  Doesn't have to be
  full ai2thor scenario, but maybe a flattented simulation.

For 1 and 2, we can pretrain a model and fine-tune to specific task.
Might be a good idea in general -- pretrain some large model that can
be fine-tuned.  We can report on the relationship between the amount
of finetuning data and performance.  

--------------------------------------------------------------------------------
TOKENIZATION

It is somewhat tempting to try to train a BPE tokenizer; there are
ways in which this does and does not make sense.  It doesn't make
sense because basic-level tokens are determined in a formal language;
they correspond to the elements of the formal grammar.  On the other
hand, the BPE encoding can help us determine when to combine tokens
into higher level tokens.  E.g.  perhaps "right(X)" will be determined
to want its own tokenization because it occurs so frequently.

One issue that may or may not be dealt with using ML methods is that
there are several isomorphic forms of a single sentence.  even
something as simple as "forall x, q(x) --> r(x)" being the same as
"forall z, q(z) --> r(z)".  Long-term, there may be a case for
standardizing all the sentences that the network has to work with.
This might look like some combination of putting sentences into normal
form along with a way to normalize variable names.   For the latter
issue, there is some reason to hope that the network will learn the
semantics of variables names well enough to obviate the need for
this.  For that matter, this could be true of the syntactic form as
well.  [THIS WOULD BE A GOOD THING TO TEST AND DEMONSTRATE EMPIRICALLY
IN THE FINAL PAPER].




--------------------------------------------------------------------------------

ENCODER-DECODER MODEL.

Some random thoughts

*  The next sentence prediciton task corresponds exactly to learning
   whether sentence 2 is a logical consequence of the KB.  This seems
   like a good sign.

   For the sentence pair encoding task, the right way to do the
   tokenization seems to be
   		<s> KB(t) </s> </s> KB(t+1) </s>
     *Perhaps we want to the individual sentences in the KB to have their
      own seperator tokens.
      
     * Also, we may want the new tokens in KB(t+1) to be listed first?
       This is to avoid falling out of the context window

     * In this context, it seems like it could be helpful to strip the
       timestamp info out as this might distract from finding larger
       structure.   Ultimately should try it both ways.  Similarly get
       rid of walltime() predicates (always) and now() predicates (as
       an experiment).

    
       
   

*  Ideally we should be invariant to the order of the sentences (but
   not the order of the tokens within each sentence.   This is maybe
   something to look at for secon round of the model.

   That said, the premise of ALMA is somewhat that when a formula is
   introduced *does* matter.  So it might be an interesting experiment
   to compare a totally order agnostic representation to one in which,
   e.g., the timestamp is encoded as part of the sentence.


--------------------------------------------------------------------------------
THE ENVIRONMENT

    * We are likely to need massive amounts of data, and it won't do
      to simply live with the memory leaks in the python alma module.
      One approach might be to try to fix the leaks; a hack might be
      to wrap alma in some kind of server the periodically restarts to
      clear leaks.  

--------------------------------------------------------------------------------
VARIATIONS
   *  With the encoder, first pass will encode knowledge base and sentence completion
      training will try to predict a viable consequence of the knowledge base.
      It would be interesting to compare performance if we try to predict an
      (action, consequence) pair instead.

   *  Try different encodings.   For example, it would be interesting
      to have a 1-1 correspondence between tokenizations and
      sentences.  It seems we lose this immediately because there are
      different embeddings for different numbers of parens.  Would it
      help to have some kind of prefix code?  It would be interesting
      to compares infix, prefix and postfix.  
